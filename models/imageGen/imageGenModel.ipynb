{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc798502-b823-402e-8a2b-8ba40308be4d",
   "metadata": {},
   "source": [
    "# TEST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7897ca73-283c-430e-902c-073a865c7517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to C:\\Users\\intelaipc\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Login using your token\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f4b56ef-32b3-45c6-a449-4e000080bc9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cae894e8d844097ae1fbe83f15fbb19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 23 files:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d94ff336cb2d46eaba658c5aacb8576e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:  22%|##1       | 986M/4.53G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbf3a289bd0e476ab5b72eb2e82f0d48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)pytorch_model-00002-of-00003.safetensors:  10%|9         | 975M/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9e75870655e4b54872736dd4bc57ea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:  19%|#9        | 954M/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3deb9e5bd17a4bb6ac5b3f02e18d5831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)pytorch_model-00001-of-00003.safetensors:  10%|#         | 1.01G/9.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d432934d4dec44fca05530ff8be04233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)pytorch_model-00003-of-00003.safetensors:  31%|###       | 1.18G/3.87G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import FluxPipeline\n",
    "\n",
    "pipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-schnell\", torch_dtype=torch.bfloat16)\n",
    "pipe.enable_model_cpu_offload() #save some VRAM by offloading the model to CPU. Remove this if you have enough GPU power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212709aa-b8f4-42b2-b4ae-20a06181b1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Children's book illustration of a wizard petting a  deer\"\n",
    "image = pipe(\n",
    "    prompt,\n",
    "    guidance_scale=0.0,\n",
    "    num_inference_steps=4,\n",
    "    max_sequence_length=256,\n",
    "    generator=torch.Generator(\"cpu\").manual_seed(0)\n",
    ").images[0]\n",
    "\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34081aa",
   "metadata": {},
   "source": [
    "## SPEED UP INFERENCE: using bfloat16\n",
    "\n",
    "https://huggingface.co/docs/diffusers/en/tutorials/fast_diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310b41c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip3 install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73834c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import diffusers\n",
    "from diffusers import StableDiffusionXLPipeline\n",
    "import torch\n",
    "\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-schnell\", torch_dtype=torch.bfloat16\n",
    ").to(\"CPU\")\n",
    "\n",
    "# Run the attention ops without SDPA.\n",
    "pipe.unet.set_default_attn_processor()\n",
    "pipe.vae.set_default_attn_processor()\n",
    "\n",
    "prompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n",
    "image = pipe(prompt, num_inference_steps=30).images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ecb935-f396-422e-ae68-1274eb4a4270",
   "metadata": {},
   "source": [
    "# QUANTIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e01fa0b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce1e5c9cac34e0aa199996eb019d00f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 23 files:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b755f65f5efe4d92a0b7884951fcaf66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)pytorch_model-00002-of-00003.safetensors:  10%|#         | 1.01G/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "717d6941daec44b585cf497a483c0a44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:  20%|#9        | 975M/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "383d68f8f7e34d95a08754ea89f42c6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)pytorch_model-00003-of-00003.safetensors:  31%|###1      | 1.21G/3.87G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff291c9fdc8450ba1e00d6125f48a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)pytorch_model-00001-of-00003.safetensors:  11%|#         | 1.06G/9.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "406b510c1b4a4bb5a6e3b99b610996b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:  22%|##2       | 1.01G/4.53G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import FluxPipeline\n",
    "\n",
    "# Load the model\n",
    "pipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-schnell\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "# Print model configuration to understand expected input dimensions\n",
    "print(pipe.config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1211fdfb",
   "metadata": {},
   "source": [
    "Convert the Model to ONNX\n",
    "First, convert the Hugging Face model to ONNX format. This can be done using the transformers library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa88869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel\n",
    "\n",
    "# Load the model components from the Hugging Face Hub\n",
    "unet = UNet2DConditionModel.from_pretrained(\"black-forest-labs/FLUX.1-schnell\")\n",
    "vae = AutoencoderKL.from_pretrained(\"black-forest-labs/FLUX.1-schnell\")\n",
    "\n",
    "# Example inputs for conversion (adjust sizes as needed for your model)\n",
    "input_ids = torch.randn(1, 4, 64, 64)  # Example input shape, modify as needed\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "unet.eval()\n",
    "vae.eval()\n",
    "\n",
    "# Function to convert a PyTorch model to ONNX\n",
    "def convert_to_onnx(model, example_input, file_name):\n",
    "    torch.onnx.export(model, \n",
    "                      example_input, \n",
    "                      file_name,\n",
    "                      export_params=True,\n",
    "                      opset_version=12,\n",
    "                      do_constant_folding=True,\n",
    "                      input_names=['input'],\n",
    "                      output_names=['output'],\n",
    "                      dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}})\n",
    "\n",
    "# Convert UNet and VAE to ONNX\n",
    "convert_to_onnx(unet, input_ids, \"unet_model.onnx\")\n",
    "convert_to_onnx(vae, input_ids, \"vae_model.onnx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66a4cd6",
   "metadata": {},
   "source": [
    "Convert ONNX Model to OpenVINO IR Format\n",
    "Use the OpenVINO Model Optimizer to convert the ONNX model to OpenVINO's IR format. This is done via the command line:\n",
    "python \"C:\\Program Files (x86)\\Intel\\openvino_2022\\deployment_tools\\model_optimizer\\mo.py\" \\\n",
    "      --input_model path_to_save/model.onnx \\\n",
    "      --output_dir path_to_save/optimized_model \\\n",
    "      --input_shape [1,64]  # Adjust the input shape based on the model's requirements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b353ebc3",
   "metadata": {},
   "source": [
    "Quantize the Model\n",
    "Quantize the model using the OpenVINO Post-Training Optimization Tool (POT). First, create a JSON configuration file for the quantization process (config.json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357387c6",
   "metadata": {},
   "source": [
    "quantize the model:\n",
    "\n",
    "pot -c path_to_quantization_config.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intel-NUC-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
